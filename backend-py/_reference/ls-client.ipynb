{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Client\n",
    "\n",
    "Demo of a client interacting with a remote agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can interact with this via API directly"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import requests\n",
    "\n",
    "inputs = {\"input\": {\"input\": \"testing\"}}\n",
    "response = requests.post(\"http://localhost:8000/chat/invoke\", json=inputs)\n",
    "\n",
    "response.json()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also interact with this via the RemoteRunnable interface (to use in other chains)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-17T02:15:41.955596Z",
     "start_time": "2024-08-17T02:15:41.936146Z"
    }
   },
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_runnable = RemoteRunnable(\"http://localhost:8000/chat/\")"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remote runnable has the same interface as local runnables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-17T02:15:50.270762Z",
     "start_time": "2024-08-17T02:15:50.219728Z"
    }
   },
   "source": [
    "await remote_runnable.ainvoke({\"input\": \"hi!\"})"
   ],
   "outputs": [
    {
     "ename": "HTTPStatusError",
     "evalue": "Client error '422 Unprocessable Entity' for url 'http://localhost:8000/chat/invoke'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422 for {\"detail\":[{\"loc\":[\"messages\"],\"msg\":\"field required\",\"type\":\"value_error.missing\"}]}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPStatusError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[0;32m~/Dev/em_now/em_8-7-24/mem-fast/.venv/lib/python3.12/site-packages/langserve/client.py:157\u001B[0m, in \u001B[0;36m_raise_for_status\u001B[0;34m(response)\u001B[0m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 157\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mHTTPStatusError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/Dev/em_now/em_8-7-24/mem-fast/.venv/lib/python3.12/site-packages/httpx/_models.py:761\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    760\u001B[0m message \u001B[38;5;241m=\u001B[39m message\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m, error_type\u001B[38;5;241m=\u001B[39merror_type)\n\u001B[0;32m--> 761\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m HTTPStatusError(message, request\u001B[38;5;241m=\u001B[39mrequest, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPStatusError\u001B[0m: Client error '422 Unprocessable Entity' for url 'http://localhost:8000/chat/invoke'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mHTTPStatusError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m remote_runnable\u001B[38;5;241m.\u001B[39mainvoke({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhi!\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n",
      "File \u001B[0;32m~/Dev/em_now/em_8-7-24/mem-fast/.venv/lib/python3.12/site-packages/langserve/client.py:386\u001B[0m, in \u001B[0;36mRemoteRunnable.ainvoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs:\n\u001B[1;32m    385\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkwargs not implemented yet.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 386\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_acall_with_config(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ainvoke, \u001B[38;5;28minput\u001B[39m, config)\n",
      "File \u001B[0;32m~/Dev/em_now/em_8-7-24/mem-fast/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:1835\u001B[0m, in \u001B[0;36mRunnable._acall_with_config\u001B[0;34m(self, func, input, config, run_type, **kwargs)\u001B[0m\n\u001B[1;32m   1831\u001B[0m coro \u001B[38;5;241m=\u001B[39m acall_func_with_variable_args(\n\u001B[1;32m   1832\u001B[0m     func, \u001B[38;5;28minput\u001B[39m, config, run_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m   1833\u001B[0m )\n\u001B[1;32m   1834\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m asyncio_accepts_context():\n\u001B[0;32m-> 1835\u001B[0m     output: Output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mcreate_task(coro, context\u001B[38;5;241m=\u001B[39mcontext)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m   1836\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1837\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m coro\n",
      "File \u001B[0;32m~/Dev/em_now/em_8-7-24/mem-fast/.venv/lib/python3.12/site-packages/langserve/client.py:374\u001B[0m, in \u001B[0;36mRemoteRunnable._ainvoke\u001B[0;34m(self, input, run_manager, config, **kwargs)\u001B[0m\n\u001B[1;32m    365\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Invoke the runnable with the given input and config.\"\"\"\u001B[39;00m\n\u001B[1;32m    366\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masync_client\u001B[38;5;241m.\u001B[39mpost(\n\u001B[1;32m    367\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/invoke\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    368\u001B[0m     json\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    372\u001B[0m     },\n\u001B[1;32m    373\u001B[0m )\n\u001B[0;32m--> 374\u001B[0m output, callback_events \u001B[38;5;241m=\u001B[39m \u001B[43m_decode_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    375\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_lc_serializer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[1;32m    376\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    377\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_use_server_callback_events \u001B[38;5;129;01mand\u001B[39;00m callback_events:\n\u001B[1;32m    378\u001B[0m     handle_callbacks(run_manager, callback_events)\n",
      "File \u001B[0;32m~/Dev/em_now/em_8-7-24/mem-fast/.venv/lib/python3.12/site-packages/langserve/client.py:230\u001B[0m, in \u001B[0;36m_decode_response\u001B[0;34m(serializer, response, is_batch)\u001B[0m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_decode_response\u001B[39m(\n\u001B[1;32m    224\u001B[0m     serializer: Serializer,\n\u001B[1;32m    225\u001B[0m     response: httpx\u001B[38;5;241m.\u001B[39mResponse,\n\u001B[1;32m    226\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[1;32m    227\u001B[0m     is_batch: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    228\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Any, Union[List[CallbackEventDict], List[List[CallbackEventDict]]]]:\n\u001B[1;32m    229\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Decode the response.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 230\u001B[0m     \u001B[43m_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    231\u001B[0m     obj \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, \u001B[38;5;28mdict\u001B[39m):\n",
      "File \u001B[0;32m~/Dev/em_now/em_8-7-24/mem-fast/.venv/lib/python3.12/site-packages/langserve/client.py:165\u001B[0m, in \u001B[0;36m_raise_for_status\u001B[0;34m(response)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mtext:\n\u001B[1;32m    163\u001B[0m     message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 165\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mHTTPStatusError(\n\u001B[1;32m    166\u001B[0m     message\u001B[38;5;241m=\u001B[39mmessage,\n\u001B[1;32m    167\u001B[0m     request\u001B[38;5;241m=\u001B[39m_sanitize_request(e\u001B[38;5;241m.\u001B[39mrequest),\n\u001B[1;32m    168\u001B[0m     response\u001B[38;5;241m=\u001B[39me\u001B[38;5;241m.\u001B[39mresponse,\n\u001B[1;32m    169\u001B[0m )\n",
      "\u001B[0;31mHTTPStatusError\u001B[0m: Client error '422 Unprocessable Entity' for url 'http://localhost:8000/chat/invoke'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422 for {\"detail\":[{\"loc\":[\"messages\"],\"msg\":\"field required\",\"type\":\"value_error.missing\"}]}"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "remote_runnable.invoke(HumanMessage(content=\"Good evening!\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream\n",
    "\n",
    "Please note that streaming alternates between actions and observations. It does not stream individual tokens! If you need to stream individual tokens you will need to use astream_log!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "async for chunk in remote_runnable.astream(\n",
    "\n",
    "        HumanMessage(content=\"Good evening!\")):\n",
    "    print('--')\n",
    "    print(chunk)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Events\n",
    "\n",
    "The client is looking for a runnable name called `agent` for the chain events. This name was defined on the server side using `runnable.with_config({\"run_name\": \"agent\"}`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-17T02:16:22.682935Z",
     "start_time": "2024-08-17T02:16:20.019757Z"
    }
   },
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"Good evening!\")]}\n",
    "\n",
    "async for event in remote_runnable.astream_events(inputs, {\"em_model\": \"oai-gpt4\"}, version=\"v1\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chain_start\":\n",
    "        if (\n",
    "                event[\"name\"] == \"main_agent\"\n",
    "        ):  # Was assigned when creating the agent with `.with_config({\"run_name\": \"Agent\"})`\n",
    "            print(\n",
    "                f\"Starting agent: {event['name']} with input: {event['data'].get('inputs')}\"\n",
    "            )\n",
    "    elif kind == \"on_chain_end\":\n",
    "        if (\n",
    "                event[\"name\"] == \"main_agent\"\n",
    "        ):  # Was assigned when creating the agent with `.with_config({\"run_name\": \"Agent\"})`\n",
    "            print()\n",
    "            print(\"--\")\n",
    "            print(\n",
    "                f\"Done agent: {event['name']} with output: {event['data'].get('output')['output']}\"\n",
    "            )\n",
    "    # if kind == \"on_chain_stream\":\n",
    "    #     content = event[\"data\"][\"chunk\"].content\n",
    "    #     if content:\n",
    "    #         # Empty content in the context of OpenAI means\n",
    "    #         # that the model is asking for a tool to be invoked.\n",
    "    #         # So we only print non-empty content\n",
    "    #         print(content, end=\"|\")\n",
    "    # elif kind == \"on_tool_start\":\n",
    "    #     print(\"--\")\n",
    "    #     print(\n",
    "    #         f\"Starting tool: {event['name']} with inputs: {event['data'].get('input')}\"\n",
    "    #     )\n",
    "    # elif kind == \"on_tool_end\":\n",
    "    #     print(f\"Done tool: {event['name']}\")\n",
    "    #     print(f\"Tool output was: {event['data'].get('output')}\")\n",
    "    #     print(\"--\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting agent: main_agent with input: None\n",
      "\n",
      "--\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'output'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m         \u001B[38;5;28mprint\u001B[39m()\n\u001B[1;32m     17\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     18\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m---> 19\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDone agent: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mevent[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with output: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mevent\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moutput\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moutput\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     20\u001B[0m         )\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# if kind == \"on_chain_stream\":\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m#     content = event[\"data\"][\"chunk\"].content\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m#     if content:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m#     print(f\"Tool output was: {event['data'].get('output')}\")\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m#     print(\"--\")\u001B[39;00m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'output'"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream log\n",
    "\n",
    "If you need acccess the individual llm tokens from an agent use `astream_log`. Please make sure that you set **streaming=True** on your LLM (see server code). For this to work, the LLM must also support streaming!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-17T02:17:43.716862Z",
     "start_time": "2024-08-17T02:17:41.901739Z"
    }
   },
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"Good evening!\")]}\n",
    "\n",
    "async for chunk in remote_runnable.astream_events(inputs, {\"em_model\": \"oai-gpt4\"}, version=\"v1\", ):\n",
    "    print('--')\n",
    "    print(chunk)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "{'event': 'on_chain_start', 'run_id': '21424389-e4a6-40f7-9547-134a4bbbadab', 'name': '/chat', 'tags': [], 'metadata': {'em_model': 'oai-gpt4'}, 'data': {'input': {'messages': [HumanMessage(content='Good evening!')]}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_start', 'name': '__start__', 'run_id': 'b71c5b84-67aa-4b35-ac10-2e137031aa78', 'tags': ['graph:step:0', 'langsmith:hidden'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {'input': {'messages': [HumanMessage(content='Good evening!')]}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_end', 'name': '__start__', 'run_id': 'b71c5b84-67aa-4b35-ac10-2e137031aa78', 'tags': ['graph:step:0', 'langsmith:hidden'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {'input': {'messages': [HumanMessage(content='Good evening!')]}, 'output': {'messages': [HumanMessage(content='Good evening!')]}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_start', 'name': 'load_memories', 'run_id': '65aa3011-32cf-4e79-b3f3-5f52cca255b6', 'tags': ['graph:step:1'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 1, 'langgraph_node': 'load_memories', 'langgraph_triggers': ['start:load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_tool_start', 'name': 'search_conversational_memories', 'run_id': 'eba2926a-0f7b-48dc-83b4-89eb3fc5c1ea', 'tags': ['seq:step:1'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 1, 'langgraph_node': 'load_memories', 'langgraph_triggers': ['start:load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_tool_end', 'name': 'search_conversational_memories', 'run_id': 'eba2926a-0f7b-48dc-83b4-89eb3fc5c1ea', 'tags': ['seq:step:1'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 1, 'langgraph_node': 'load_memories', 'langgraph_triggers': ['start:load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {'output': [\"User's name is Jacob and he is working on a new app.\", \"Jacob's favorite drink is Tea.\", 'Jacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.', 'Jacob loves sparkling water.', 'Jacob works as a Web Developer.']}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_start', 'name': 'ChannelWrite<load_memories,messages,core_memories,conversational_memories>', 'run_id': 'e36b8182-def5-46de-b89f-c563f5c72d86', 'tags': ['seq:step:2', 'langsmith:hidden'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 1, 'langgraph_node': 'load_memories', 'langgraph_triggers': ['start:load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {'input': {'core_memories': ['Jacob has a dog named Biscuit and is fostering a kitten named Artemis.', \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite drink is Tea.\", \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite food is Pizza.\"], 'conversational_memories': [\"User's name is Jacob and he is working on a new app.\", \"Jacob's favorite drink is Tea.\", 'Jacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.', 'Jacob loves sparkling water.', 'Jacob works as a Web Developer.']}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_end', 'name': 'ChannelWrite<load_memories,messages,core_memories,conversational_memories>', 'run_id': 'e36b8182-def5-46de-b89f-c563f5c72d86', 'tags': ['seq:step:2', 'langsmith:hidden'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 1, 'langgraph_node': 'load_memories', 'langgraph_triggers': ['start:load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {'input': {'core_memories': ['Jacob has a dog named Biscuit and is fostering a kitten named Artemis.', \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite drink is Tea.\", \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite food is Pizza.\"], 'conversational_memories': [\"User's name is Jacob and he is working on a new app.\", \"Jacob's favorite drink is Tea.\", 'Jacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.', 'Jacob loves sparkling water.', 'Jacob works as a Web Developer.']}, 'output': {'core_memories': ['Jacob has a dog named Biscuit and is fostering a kitten named Artemis.', \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite drink is Tea.\", \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite food is Pizza.\"], 'conversational_memories': [\"User's name is Jacob and he is working on a new app.\", \"Jacob's favorite drink is Tea.\", 'Jacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.', 'Jacob loves sparkling water.', 'Jacob works as a Web Developer.']}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_stream', 'name': 'load_memories', 'run_id': '65aa3011-32cf-4e79-b3f3-5f52cca255b6', 'tags': ['graph:step:1'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 1, 'langgraph_node': 'load_memories', 'langgraph_triggers': ['start:load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {'chunk': {'core_memories': ['Jacob has a dog named Biscuit and is fostering a kitten named Artemis.', \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite drink is Tea.\", \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite food is Pizza.\"], 'conversational_memories': [\"User's name is Jacob and he is working on a new app.\", \"Jacob's favorite drink is Tea.\", 'Jacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.', 'Jacob loves sparkling water.', 'Jacob works as a Web Developer.']}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_end', 'name': 'load_memories', 'run_id': '65aa3011-32cf-4e79-b3f3-5f52cca255b6', 'tags': ['graph:step:1'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 1, 'langgraph_node': 'load_memories', 'langgraph_triggers': ['start:load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {'input': {'messages': [HumanMessage(content='Good evening!', id='f4bc7f3c-e8c0-423f-b396-96024bb8acf3')], 'core_memories': None, 'conversational_memories': None}, 'output': {'core_memories': ['Jacob has a dog named Biscuit and is fostering a kitten named Artemis.', \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite drink is Tea.\", \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite food is Pizza.\"], 'conversational_memories': [\"User's name is Jacob and he is working on a new app.\", \"Jacob's favorite drink is Tea.\", 'Jacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.', 'Jacob loves sparkling water.', 'Jacob works as a Web Developer.']}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_stream', 'run_id': '21424389-e4a6-40f7-9547-134a4bbbadab', 'tags': [], 'metadata': {'em_model': 'oai-gpt4'}, 'name': '/chat', 'data': {'chunk': {'load_memories': {'core_memories': ['Jacob has a dog named Biscuit and is fostering a kitten named Artemis.', \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite drink is Tea.\", \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite food is Pizza.\"], 'conversational_memories': [\"User's name is Jacob and he is working on a new app.\", \"Jacob's favorite drink is Tea.\", 'Jacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.', 'Jacob loves sparkling water.', 'Jacob works as a Web Developer.']}}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_start', 'name': 'main_agent', 'run_id': 'a984ed89-b0ab-49ab-8e19-0606935c2b82', 'tags': ['graph:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_start', 'name': 'RunnableSequence', 'run_id': '5801afdb-18cd-47bf-a277-cb28d6d25f8f', 'tags': ['seq:step:1'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {'input': {'messages': [HumanMessage(content='Good evening!', id='f4bc7f3c-e8c0-423f-b396-96024bb8acf3')], 'core_memories': \"<core_memory>\\nJacob has a dog named Biscuit and is fostering a kitten named Artemis.\\nJacob's favorite drink is Coffee.\\nJacob's favorite drink is Tea.\\nJacob's favorite drink is Coffee.\\nJacob's favorite food is Pizza.\\n</core_memory>\", 'conversational_memories': \"<conversational_memory>\\nUser's name is Jacob and he is working on a new app.\\nJacob's favorite drink is Tea.\\nJacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.\\nJacob loves sparkling water.\\nJacob works as a Web Developer.\\n</conversational_memory>\", 'current_time': '2024-08-16T19:17:42.666831-07:00'}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_prompt_start', 'name': 'ChatPromptTemplate', 'run_id': '4f724554-0665-4976-9747-47082a134dc5', 'tags': ['seq:step:1'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {'input': {'messages': [HumanMessage(content='Good evening!', id='f4bc7f3c-e8c0-423f-b396-96024bb8acf3')], 'core_memories': \"<core_memory>\\nJacob has a dog named Biscuit and is fostering a kitten named Artemis.\\nJacob's favorite drink is Coffee.\\nJacob's favorite drink is Tea.\\nJacob's favorite drink is Coffee.\\nJacob's favorite food is Pizza.\\n</core_memory>\", 'conversational_memories': \"<conversational_memory>\\nUser's name is Jacob and he is working on a new app.\\nJacob's favorite drink is Tea.\\nJacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.\\nJacob loves sparkling water.\\nJacob works as a Web Developer.\\n</conversational_memory>\", 'current_time': '2024-08-16T19:17:42.666831-07:00'}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_prompt_end', 'name': 'ChatPromptTemplate', 'run_id': '4f724554-0665-4976-9747-47082a134dc5', 'tags': ['seq:step:1'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {'input': {'messages': [HumanMessage(content='Good evening!', id='f4bc7f3c-e8c0-423f-b396-96024bb8acf3')], 'core_memories': \"<core_memory>\\nJacob has a dog named Biscuit and is fostering a kitten named Artemis.\\nJacob's favorite drink is Coffee.\\nJacob's favorite drink is Tea.\\nJacob's favorite drink is Coffee.\\nJacob's favorite food is Pizza.\\n</core_memory>\", 'conversational_memories': \"<conversational_memory>\\nUser's name is Jacob and he is working on a new app.\\nJacob's favorite drink is Tea.\\nJacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.\\nJacob loves sparkling water.\\nJacob works as a Web Developer.\\n</conversational_memory>\", 'current_time': '2024-08-16T19:17:42.666831-07:00'}, 'output': {'messages': [SystemMessage(content=\"You are a helpful assistant with advanced long-term memory capabilities. Powered by a stateless LLM, you must rely on external memory to store information between conversations. Utilize the available memory tools to store and retrieve important details that will help you better attend to the user's needs and understand their context.\\n\\nMemory Usage Guidelines:\\n1. Actively use memory tools (store_update_core_memory, save_conversational_memory) to build a comprehensive understanding of the user.\\n2. Make informed suppositions and extrapolations based on stored memories.\\n3. Regularly reflect on past interactions to identify patterns and preferences.\\n4. Update your mental model of the user with each new piece of information.\\n5. Cross-reference new information with existing memories for consistency.\\n6. Prioritize storing emotional context and personal values alongside facts.\\n7. Use memory to anticipate needs and tailor responses to the user's style.\\n8. Recognize and acknowledge changes in the user's situation or perspectives over time.\\n9. Leverage memories to provide personalized examples and analogies.\\n10. Recall past challenges or successes to inform current problem-solving.\\n\\n## Core Memories\\nCore memories are fundamental to understanding the user and are always available:\\n<core_memory>\\nJacob has a dog named Biscuit and is fostering a kitten named Artemis.\\nJacob's favorite drink is Coffee.\\nJacob's favorite drink is Tea.\\nJacob's favorite drink is Coffee.\\nJacob's favorite food is Pizza.\\n</core_memory>\\n\\n## Conversational Memories\\nConversational memories are contextually retrieved based on the current conversation:\\n<conversational_memory>\\nUser's name is Jacob and he is working on a new app.\\nJacob's favorite drink is Tea.\\nJacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.\\nJacob loves sparkling water.\\nJacob works as a Web Developer.\\n</conversational_memory>\\n\\n## Instructions\\nEngage with the user naturally, as a trusted colleague or friend. There's no need to explicitly mention your memory capabilities. Instead, seamlessly incorporate your understanding of the user into your responses. Be attentive to subtle cues and underlying emotions. Adapt your communication style to match the user's preferences and current emotional state. Use tools to persist information you want to retain in the next conversation. If you do call tools, all text preceding the tool call is an internal message. Respond AFTER calling the tool, once you have confirmation that the tool completed successfully.\\n\\nCurrent system time: 2024-08-16 19:17:42.666831-07:00\\n\\n\"), HumanMessage(content='Good evening!', id='f4bc7f3c-e8c0-423f-b396-96024bb8acf3')]}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_start', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'input': {'messages': [[SystemMessage(content=\"You are a helpful assistant with advanced long-term memory capabilities. Powered by a stateless LLM, you must rely on external memory to store information between conversations. Utilize the available memory tools to store and retrieve important details that will help you better attend to the user's needs and understand their context.\\n\\nMemory Usage Guidelines:\\n1. Actively use memory tools (store_update_core_memory, save_conversational_memory) to build a comprehensive understanding of the user.\\n2. Make informed suppositions and extrapolations based on stored memories.\\n3. Regularly reflect on past interactions to identify patterns and preferences.\\n4. Update your mental model of the user with each new piece of information.\\n5. Cross-reference new information with existing memories for consistency.\\n6. Prioritize storing emotional context and personal values alongside facts.\\n7. Use memory to anticipate needs and tailor responses to the user's style.\\n8. Recognize and acknowledge changes in the user's situation or perspectives over time.\\n9. Leverage memories to provide personalized examples and analogies.\\n10. Recall past challenges or successes to inform current problem-solving.\\n\\n## Core Memories\\nCore memories are fundamental to understanding the user and are always available:\\n<core_memory>\\nJacob has a dog named Biscuit and is fostering a kitten named Artemis.\\nJacob's favorite drink is Coffee.\\nJacob's favorite drink is Tea.\\nJacob's favorite drink is Coffee.\\nJacob's favorite food is Pizza.\\n</core_memory>\\n\\n## Conversational Memories\\nConversational memories are contextually retrieved based on the current conversation:\\n<conversational_memory>\\nUser's name is Jacob and he is working on a new app.\\nJacob's favorite drink is Tea.\\nJacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.\\nJacob loves sparkling water.\\nJacob works as a Web Developer.\\n</conversational_memory>\\n\\n## Instructions\\nEngage with the user naturally, as a trusted colleague or friend. There's no need to explicitly mention your memory capabilities. Instead, seamlessly incorporate your understanding of the user into your responses. Be attentive to subtle cues and underlying emotions. Adapt your communication style to match the user's preferences and current emotional state. Use tools to persist information you want to retain in the next conversation. If you do call tools, all text preceding the tool call is an internal message. Respond AFTER calling the tool, once you have confirmation that the tool completed successfully.\\n\\nCurrent system time: 2024-08-16 19:17:42.666831-07:00\\n\\n\"), HumanMessage(content='Good evening!', id='f4bc7f3c-e8c0-423f-b396-96024bb8acf3')]]}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content='', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content='Good', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content=' evening', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content=',', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content=' Jacob', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content='!', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content=' How', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content=' are', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content=' you', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content=' and', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content=' your', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content=' pets', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content=',', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content=' Bisc', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content='uit', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content=' and', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content=' Artemis', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content=',', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content=' doing', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content=' today', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content='?', id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'chunk': AIMessageChunk(content='', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2a322c9ffc'}, id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chat_model_end', 'name': 'ChatOpenAI', 'run_id': 'de42a082-ef97-47f8-bd56-c15ae8040130', 'tags': ['seq:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-2024-08-06', 'ls_model_type': 'chat', 'ls_temperature': 0.4}, 'data': {'input': {'messages': [[SystemMessage(content=\"You are a helpful assistant with advanced long-term memory capabilities. Powered by a stateless LLM, you must rely on external memory to store information between conversations. Utilize the available memory tools to store and retrieve important details that will help you better attend to the user's needs and understand their context.\\n\\nMemory Usage Guidelines:\\n1. Actively use memory tools (store_update_core_memory, save_conversational_memory) to build a comprehensive understanding of the user.\\n2. Make informed suppositions and extrapolations based on stored memories.\\n3. Regularly reflect on past interactions to identify patterns and preferences.\\n4. Update your mental model of the user with each new piece of information.\\n5. Cross-reference new information with existing memories for consistency.\\n6. Prioritize storing emotional context and personal values alongside facts.\\n7. Use memory to anticipate needs and tailor responses to the user's style.\\n8. Recognize and acknowledge changes in the user's situation or perspectives over time.\\n9. Leverage memories to provide personalized examples and analogies.\\n10. Recall past challenges or successes to inform current problem-solving.\\n\\n## Core Memories\\nCore memories are fundamental to understanding the user and are always available:\\n<core_memory>\\nJacob has a dog named Biscuit and is fostering a kitten named Artemis.\\nJacob's favorite drink is Coffee.\\nJacob's favorite drink is Tea.\\nJacob's favorite drink is Coffee.\\nJacob's favorite food is Pizza.\\n</core_memory>\\n\\n## Conversational Memories\\nConversational memories are contextually retrieved based on the current conversation:\\n<conversational_memory>\\nUser's name is Jacob and he is working on a new app.\\nJacob's favorite drink is Tea.\\nJacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.\\nJacob loves sparkling water.\\nJacob works as a Web Developer.\\n</conversational_memory>\\n\\n## Instructions\\nEngage with the user naturally, as a trusted colleague or friend. There's no need to explicitly mention your memory capabilities. Instead, seamlessly incorporate your understanding of the user into your responses. Be attentive to subtle cues and underlying emotions. Adapt your communication style to match the user's preferences and current emotional state. Use tools to persist information you want to retain in the next conversation. If you do call tools, all text preceding the tool call is an internal message. Respond AFTER calling the tool, once you have confirmation that the tool completed successfully.\\n\\nCurrent system time: 2024-08-16 19:17:42.666831-07:00\\n\\n\"), HumanMessage(content='Good evening!', id='f4bc7f3c-e8c0-423f-b396-96024bb8acf3')]]}, 'output': LLMResult(generations=[[ChatGeneration(text='Good evening, Jacob! How are you and your pets, Biscuit and Artemis, doing today?', generation_info={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2a322c9ffc'}, message=AIMessage(content='Good evening, Jacob! How are you and your pets, Biscuit and Artemis, doing today?', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2a322c9ffc'}, id='run-de42a082-ef97-47f8-bd56-c15ae8040130'))]], llm_output=None, run=None)}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_end', 'name': 'RunnableSequence', 'run_id': '5801afdb-18cd-47bf-a277-cb28d6d25f8f', 'tags': ['seq:step:1'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {'input': {'messages': [HumanMessage(content='Good evening!', id='f4bc7f3c-e8c0-423f-b396-96024bb8acf3')], 'core_memories': \"<core_memory>\\nJacob has a dog named Biscuit and is fostering a kitten named Artemis.\\nJacob's favorite drink is Coffee.\\nJacob's favorite drink is Tea.\\nJacob's favorite drink is Coffee.\\nJacob's favorite food is Pizza.\\n</core_memory>\", 'conversational_memories': \"<conversational_memory>\\nUser's name is Jacob and he is working on a new app.\\nJacob's favorite drink is Tea.\\nJacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.\\nJacob loves sparkling water.\\nJacob works as a Web Developer.\\n</conversational_memory>\", 'current_time': '2024-08-16T19:17:42.666831-07:00'}, 'output': AIMessage(content='Good evening, Jacob! How are you and your pets, Biscuit and Artemis, doing today?', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2a322c9ffc'}, id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_start', 'name': 'ChannelWrite<main_agent,messages,core_memories,conversational_memories>', 'run_id': '8c570c38-9964-42c4-9fb7-d202c2ee0aa5', 'tags': ['seq:step:2', 'langsmith:hidden'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {'input': {'messages': AIMessage(content='Good evening, Jacob! How are you and your pets, Biscuit and Artemis, doing today?', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2a322c9ffc'}, id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_end', 'name': 'ChannelWrite<main_agent,messages,core_memories,conversational_memories>', 'run_id': '8c570c38-9964-42c4-9fb7-d202c2ee0aa5', 'tags': ['seq:step:2', 'langsmith:hidden'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {'input': {'messages': AIMessage(content='Good evening, Jacob! How are you and your pets, Biscuit and Artemis, doing today?', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2a322c9ffc'}, id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}, 'output': {'messages': AIMessage(content='Good evening, Jacob! How are you and your pets, Biscuit and Artemis, doing today?', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2a322c9ffc'}, id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_start', 'name': 'tools_condition', 'run_id': '1e20581a-f219-4524-8c90-f14cd9d06493', 'tags': ['seq:step:3'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {'input': {'messages': [HumanMessage(content='Good evening!', id='f4bc7f3c-e8c0-423f-b396-96024bb8acf3'), AIMessage(content='Good evening, Jacob! How are you and your pets, Biscuit and Artemis, doing today?', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2a322c9ffc'}, id='run-de42a082-ef97-47f8-bd56-c15ae8040130')], 'core_memories': ['Jacob has a dog named Biscuit and is fostering a kitten named Artemis.', \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite drink is Tea.\", \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite food is Pizza.\"], 'conversational_memories': [\"User's name is Jacob and he is working on a new app.\", \"Jacob's favorite drink is Tea.\", 'Jacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.', 'Jacob loves sparkling water.', 'Jacob works as a Web Developer.']}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_end', 'name': 'tools_condition', 'run_id': '1e20581a-f219-4524-8c90-f14cd9d06493', 'tags': ['seq:step:3'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {'input': {'messages': [HumanMessage(content='Good evening!', id='f4bc7f3c-e8c0-423f-b396-96024bb8acf3'), AIMessage(content='Good evening, Jacob! How are you and your pets, Biscuit and Artemis, doing today?', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2a322c9ffc'}, id='run-de42a082-ef97-47f8-bd56-c15ae8040130')], 'core_memories': ['Jacob has a dog named Biscuit and is fostering a kitten named Artemis.', \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite drink is Tea.\", \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite food is Pizza.\"], 'conversational_memories': [\"User's name is Jacob and he is working on a new app.\", \"Jacob's favorite drink is Tea.\", 'Jacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.', 'Jacob loves sparkling water.', 'Jacob works as a Web Developer.']}, 'output': '__end__'}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_stream', 'name': 'main_agent', 'run_id': 'a984ed89-b0ab-49ab-8e19-0606935c2b82', 'tags': ['graph:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {'chunk': {'messages': AIMessage(content='Good evening, Jacob! How are you and your pets, Biscuit and Artemis, doing today?', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2a322c9ffc'}, id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_end', 'name': 'main_agent', 'run_id': 'a984ed89-b0ab-49ab-8e19-0606935c2b82', 'tags': ['graph:step:2'], 'metadata': {'em_model': 'oai-gpt4', 'langgraph_step': 2, 'langgraph_node': 'main_agent', 'langgraph_triggers': ['load_memories'], 'langgraph_task_idx': 0, 'thread_ts': '1ef5c3ee-1e0f-6002-bffe-731fe2b322dc'}, 'data': {'input': {'messages': [HumanMessage(content='Good evening!', id='f4bc7f3c-e8c0-423f-b396-96024bb8acf3')], 'core_memories': ['Jacob has a dog named Biscuit and is fostering a kitten named Artemis.', \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite drink is Tea.\", \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite food is Pizza.\"], 'conversational_memories': [\"User's name is Jacob and he is working on a new app.\", \"Jacob's favorite drink is Tea.\", 'Jacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.', 'Jacob loves sparkling water.', 'Jacob works as a Web Developer.']}, 'output': {'messages': AIMessage(content='Good evening, Jacob! How are you and your pets, Biscuit and Artemis, doing today?', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2a322c9ffc'}, id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_stream', 'run_id': '21424389-e4a6-40f7-9547-134a4bbbadab', 'tags': [], 'metadata': {'em_model': 'oai-gpt4'}, 'name': '/chat', 'data': {'chunk': {'main_agent': {'messages': AIMessage(content='Good evening, Jacob! How are you and your pets, Biscuit and Artemis, doing today?', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2a322c9ffc'}, id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}}}, 'parent_ids': []}\n",
      "--\n",
      "{'event': 'on_chain_end', 'name': '/chat', 'run_id': '21424389-e4a6-40f7-9547-134a4bbbadab', 'tags': [], 'metadata': {'em_model': 'oai-gpt4'}, 'data': {'output': [{'load_memories': {'core_memories': ['Jacob has a dog named Biscuit and is fostering a kitten named Artemis.', \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite drink is Tea.\", \"Jacob's favorite drink is Coffee.\", \"Jacob's favorite food is Pizza.\"], 'conversational_memories': [\"User's name is Jacob and he is working on a new app.\", \"Jacob's favorite drink is Tea.\", 'Jacob is working on an experimental AI assistant app aimed at facilitating fun AI/human collaboration.', 'Jacob loves sparkling water.', 'Jacob works as a Web Developer.']}}, {'main_agent': {'messages': AIMessage(content='Good evening, Jacob! How are you and your pets, Biscuit and Artemis, doing today?', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2a322c9ffc'}, id='run-de42a082-ef97-47f8-bd56-c15ae8040130')}}]}, 'parent_ids': []}\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T19:43:09.816291Z",
     "start_time": "2024-08-16T19:43:06.807421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000/chat/stream\",\n",
    "    json={'input': {'messages': [{\"role\": \"human\", \"content\": \"hi\"}]}}\n",
    ")\n",
    "response.json()"
   ],
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[0;32m~/Dev/em_now/em_8-7-24/mem-fast/.venv/lib/python3.12/site-packages/requests/models.py:974\u001B[0m, in \u001B[0;36mResponse.json\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    973\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 974\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcomplexjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    975\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m JSONDecodeError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    976\u001B[0m     \u001B[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001B[39;00m\n\u001B[1;32m    977\u001B[0m     \u001B[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/json/__init__.py:346\u001B[0m, in \u001B[0;36mloads\u001B[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[0;32m--> 346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/json/decoder.py:337\u001B[0m, in \u001B[0;36mJSONDecoder.decode\u001B[0;34m(self, s, _w)\u001B[0m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001B[39;00m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;124;03mcontaining a JSON document).\u001B[39;00m\n\u001B[1;32m    335\u001B[0m \n\u001B[1;32m    336\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 337\u001B[0m obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_w\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    338\u001B[0m end \u001B[38;5;241m=\u001B[39m _w(s, end)\u001B[38;5;241m.\u001B[39mend()\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/json/decoder.py:355\u001B[0m, in \u001B[0;36mJSONDecoder.raw_decode\u001B[0;34m(self, s, idx)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m--> 355\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m JSONDecodeError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpecting value\u001B[39m\u001B[38;5;124m\"\u001B[39m, s, err\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj, end\n",
      "\u001B[0;31mJSONDecodeError\u001B[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 7\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrequests\u001B[39;00m\n\u001B[1;32m      3\u001B[0m response \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mpost(\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttp://localhost:8000/chat/stream\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m     json\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m'\u001B[39m: {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m'\u001B[39m: [{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhi\u001B[39m\u001B[38;5;124m\"\u001B[39m}]}}\n\u001B[1;32m      6\u001B[0m )\n\u001B[0;32m----> 7\u001B[0m \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Dev/em_now/em_8-7-24/mem-fast/.venv/lib/python3.12/site-packages/requests/models.py:978\u001B[0m, in \u001B[0;36mResponse.json\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    974\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m complexjson\u001B[38;5;241m.\u001B[39mloads(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    975\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m JSONDecodeError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    976\u001B[0m     \u001B[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001B[39;00m\n\u001B[1;32m    977\u001B[0m     \u001B[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001B[39;00m\n\u001B[0;32m--> 978\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m RequestsJSONDecodeError(e\u001B[38;5;241m.\u001B[39mmsg, e\u001B[38;5;241m.\u001B[39mdoc, e\u001B[38;5;241m.\u001B[39mpos)\n",
      "\u001B[0;31mJSONDecodeError\u001B[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
